import os
import io
from dotenv import load_dotenv
load_dotenv()
# â˜…â˜…â˜… è®¾ç½® Hugging Face é•œåƒæº (é˜²æ­¢ä¸‹è½½æ¨¡å‹è¶…æ—¶) â˜…â˜…â˜…
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint


from langchain_ollama import ChatOllama
from langchain_classic.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from router import init_router_chain,get_route_category
# === å¯¼å…¥æˆ‘ä»¬è‡ªå·±å†™çš„æ¨¡å— ===
from router import init_router_chain, get_route_category
from rag_engine import get_retriever_tool, advanced_rerank_search
import tools 
import re
import history_utils
# ==========================================
# ğŸ”Œ æ ¸å¿ƒå‡çº§ï¼šå¯¼å…¥åç«¯å¼•æ“
# ==========================================
# è¿™ä¸€è¡Œä»£ç å°±æŠŠ PDF è§£æã€å‘é‡åº“ã€Rerank é‡æ’åºå…¨æå®šäº†
# try:

# except ImportError:
    # st.error("âŒ æ‰¾ä¸åˆ° rag_engine.pyï¼è¯·ç¡®ä¿è¯¥æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
    # st.stop()

# ================= 2. é¡µé¢é…ç½®ä¸åˆå§‹åŒ– =================
st.set_page_config(page_title="Chaos Agent Pro", page_icon="ğŸŒªï¸", layout="wide")

# åˆå§‹åŒ– Session State (ç”¨äºå­˜å‚¨å½“å‰ä¼šè¯ä¿¡æ¯)
if "session_id" not in st.session_state:
    st.session_state.session_id = history_utils.generate_session_id()

if "messages" not in st.session_state:
    st.session_state.messages = []

# åˆå§‹åŒ– Router (åªåŠ è½½ä¸€æ¬¡)
if "router_chain" not in st.session_state:
    # è¿™é‡ŒåŠ è½½ LLM åªç”¨äºè·¯ç”±ï¼Œå¯ä»¥è½»é‡åŒ–
    llm_router = ChatOllama(model="llama3.1", temperature=0, base_url="http://127.0.0.1:11434")
    st.session_state.router_chain = init_router_chain(llm_router)

# åˆå§‹åŒ–ä¸» LLM (ç”¨äºç”Ÿæˆå›ç­”)
@st.cache_resource
def load_main_llm():
    return ChatOllama(
        model="llama3.1",
        temperature=0.3,
        keep_alive="1h",
        # base_url="http://127.0.0.1:11434"
    )

llm = load_main_llm()

# ================= 3. ä¾§è¾¹æ  (è®°å¿†åŠŸèƒ½æ ¸å¿ƒ) =================
with st.sidebar:
    st.title("ğŸ—‚ï¸ å†å²è®°å½•")
    
    # [æ–°å»ºå¯¹è¯]
    if st.button("â• æ–°å»ºå¯¹è¯", use_container_width=True):
        st.session_state.session_id = history_utils.generate_session_id()
        st.session_state.messages = []
        st.rerun()
    
    st.divider()
    
    # [å†å²åˆ—è¡¨] è¯»å– JSON æ–‡ä»¶
    sessions = history_utils.get_history_list()
    
    for sess in sessions:
        # åˆ¤æ–­æ˜¯ä¸æ˜¯å½“å‰é€‰ä¸­çš„ä¼šè¯
        is_current = (sess["id"] == st.session_state.session_id)
        btn_type = "primary" if is_current else "secondary"
        
        col1, col2 = st.columns([0.8, 0.2])
        with col1:
            # ç‚¹å‡»æ ‡é¢˜åŠ è½½å†å²
            if st.button(f"ğŸ“„ {sess['title']}", key=f"btn_{sess['id']}", type=btn_type, use_container_width=True):
                st.session_state.session_id = sess["id"]
                st.session_state.messages = history_utils.load_conversation(sess["id"])
                st.rerun()
        with col2:
            # åˆ é™¤æŒ‰é’®
            if st.button("ğŸ—‘ï¸", key=f"del_{sess['id']}"):
                history_utils.delete_conversation(sess["id"])
                if sess["id"] == st.session_state.session_id:
                    # å¦‚æœåˆ çš„æ˜¯å½“å‰ä¼šè¯ï¼Œé‡ç½®
                    st.session_state.session_id = history_utils.generate_session_id()
                    st.session_state.messages = []
                st.rerun()

    st.divider()
    st.info("ğŸ’¡ **å·¥ä½œæ¨¡å¼:**\n1. ğŸ§® æ•°å­¦ -> Python å¼•æ“\n2. ğŸ“„ ä¸“ä¸š -> æœ¬åœ°çŸ¥è¯†åº“\n3. ğŸ§  é€šç”¨ -> Llama3")

# ================= 4. ä¸»ç•Œé¢æ˜¾ç¤ºåŒºåŸŸ =================
st.title("ğŸŒªï¸ Chaos-Agent V1.0 (Hybrid Engine)")

# æ¸²æŸ“å†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ================= 5. æ ¸å¿ƒå¤„ç†é€»è¾‘ =================
if user_input := st.chat_input("è¯·è¾“å…¥é—®é¢˜ (ä¾‹å¦‚: è®¡ç®—r=3.2çš„çŠ¶æ€ / Gierer-Meinhardtæ¨¡å‹æ˜¯ä»€ä¹ˆ)..."):
    
    # [è®°å½•ç”¨æˆ·è¾“å…¥]
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # [æ„å›¾è¯†åˆ«ä¸è·¯ç”±]
    with st.status("ğŸ§  æ­£åœ¨æ€è€ƒ...", expanded=True) as status:
        category = get_route_category(user_input, st.session_state.router_chain)
        status.write(f"ğŸ·ï¸ è¯†åˆ«æ„å›¾: **{category}**")
        
        response_text = ""
        fig = None #ç”¨äºå­˜å‚¨å¯èƒ½ç”Ÿæˆçš„å›¾ç‰‡

        # â¤ åˆ†æ”¯ A: æ•°å­¦è®¡ç®— (è°ƒç”¨ tools.py)
        if category == "COMPUTE":
            status.update(label="ğŸ§® æ­£åœ¨è°ƒç”¨ Python è®¡ç®—å¼•æ“...", state="running")
            try:
                # æ­£åˆ™æå– r å€¼
                match = re.search(r"r\s*[=:]\s*(\d+\.?\d*)", user_input)
                r_val = float(match.group(1)) if match else 3.5 # é»˜è®¤å€¼
                
                if "logistic" in user_input.lower() or "æ˜ å°„" in user_input or "æ–¹ç¨‹" in user_input:
                    response_text, fig = tools.simulate_logistic_map(r_val)
                elif "lorenz" in user_input.lower() or "æ´›ä¼¦å…¹" in user_input:
                    response_text, fig = tools.simulate_lorenz()
                else:
                    response_text = "âš ï¸ æœªè¯†åˆ«å…·ä½“è®¡ç®—æ¨¡å‹ï¼Œé»˜è®¤è®¡ç®— Logistic æ˜ å°„..."
                    response_text_extra, fig = tools.simulate_logistic_map(r_val)
                    response_text += "\n" + response_text_extra
                
                # â˜…â˜…â˜… æ ¸å¿ƒä¿®å¤ï¼šå°† Matplotlib Figure è½¬ä¸ºå†…å­˜å›¾ç‰‡ â˜…â˜…â˜…
                # è¿™èƒ½é˜²æ­¢ Streamlit æŠ¥ MediaFileHandler Error
                if fig:
                    # 1. åˆ›å»ºå†…å­˜ç¼“å†²åŒº
                    buf = io.BytesIO()
                    # 2. æŠŠå›¾ä¿å­˜åˆ°ç¼“å†²åŒº
                    fig.savefig(buf, format="png", bbox_inches='tight', dpi=100)
                    # 3. æŒ‡é’ˆå½’é›¶
                    buf.seek(0)
                    # 4. æ˜¾ç¤ºå›¾ç‰‡ (ä½¿ç”¨ st.image è€Œä¸æ˜¯ st.pyplot)
                    st.image(buf, caption="Simulation Result", use_container_width=True)
                    # 5. æ˜¾å¼å…³é—­å›¾è¡¨ï¼Œé‡Šæ”¾å†…å­˜
                    plt.close(fig) 
                    
                    # (å¯é€‰) å¦‚æœä½ æƒ³æŠŠå›¾å­˜è¿›å†å²è®°å½•ï¼Œè¿™é‡Œéœ€è¦æŠŠ buf è½¬ä¸º base64 å­˜å…¥ session_state
                    # ä½†ä¸ºäº†ç®€å•ç¨³å®šï¼Œç›®å‰å†å²è®°å½•åªå­˜æ–‡å­—ï¼Œå›¾åªæ˜¾ç¤ºä¸€æ¬¡ã€‚

            except Exception as e:
                response_text = f"âŒ è®¡ç®—æ¨¡å—å‡ºé”™: {str(e)}"

        # â¤ åˆ†æ”¯ B: RAG + æ™ºèƒ½å›é€€
        elif category == "RAG":
            status.update(label="ğŸ” æ­£åœ¨æ£€ç´¢æœ¬åœ°çŸ¥è¯†åº“...", state="running")
            
            # 1. æ£€ç´¢
            retriever = get_retriever_tool()
            rag_result = retriever.func(user_input)
            
            # 2. åˆ¤åˆ«æ˜¯å¦éœ€è¦å›é€€ (Fallback)
            # å‡è®¾ rag_engine åœ¨æ²¡æœåˆ°æ—¶ä¼šè¿”å›åŒ…å«"èµ„æ–™ä¸è¶³"çš„å­—ç¬¦ä¸²ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥æ£€æŸ¥å­—ç¬¦ä¸²é•¿åº¦
            is_fallback = False
            if "èµ„æ–™ä¸è¶³" in rag_result or "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£" in rag_result:
                is_fallback = True
                status.write("âš ï¸ æœ¬åœ°åº“æœªæ”¶å½•ï¼Œ**åˆ‡æ¢è‡³é€šç”¨æ¨¡å¼**...")
            else:
                status.write("âœ… æœ¬åœ°åº“å‘½ä¸­ï¼æ­£åœ¨é˜…è¯»æ–‡çŒ®...")

            # 3. ç”Ÿæˆå›ç­”
            if is_fallback:
                prompt = ChatPromptTemplate.from_template(
                    "ç”¨æˆ·é—®é¢˜: {question}\nè¯·åˆ©ç”¨ä½ çš„é€šç”¨çŸ¥è¯†å›ç­”ã€‚å¦‚æœä¸çŸ¥é“å°±ç›´è¯´ã€‚"
                )
                chain = prompt | llm
                response_text = chain.invoke({"question": user_input}).content
                response_text += "\n\n*(æ³¨: æ­¤å›ç­”åŸºäºé€šç”¨çŸ¥è¯†ï¼Œéæœ¬åœ°æ–‡çŒ®)*"
            else:
                prompt = ChatPromptTemplate.from_template(
                    "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç ”ç©¶åŠ©æ‰‹ã€‚è¯·ä»…åŸºäºä»¥ä¸‹æ–‡çŒ®å›ç­”é—®é¢˜ï¼š\n\næ–‡çŒ®å†…å®¹:\n{context}\n\nç”¨æˆ·é—®é¢˜: {question}"
                    "åœ¨å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œå¦‚æœæ¶‰åŠåˆ°è¾“å‡ºå†…å®¹æœ‰å…¬å¼ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§Latexè¿›è¡Œå…¬å¼è¾“å‡º"
                    "åœ¨å›ç­”ç”¨æˆ·é—®é¢˜æ—¶ï¼Œå¦‚æœæ®µè½è¿‡é•¿éœ€è¦åˆ†ç‚¹å›ç­”ï¼Œè¯·åˆ†ç‚¹å›ç­”ï¼Œå¹¶æŒ‰ç…§ä¸€çº§æ ‡é¢˜-å†…å®¹è¿›è¡Œè¾“å‡º"
                )
                chain = prompt | llm
                response_text = chain.invoke({"context": rag_result, "question": user_input}).content
                # å¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸ªå‰ç¼€ï¼Œè®© UI æ›´å¥½çœ‹
                response_text = "ğŸ“š **åŸºäºæœ¬åœ°æ–‡çŒ®çš„å›ç­”ï¼š**\n\n" + response_text
        # â¤ åˆ†æ”¯ C: é—²èŠ
        else:
            status.update(label="ğŸ’¬ æ­£åœ¨ç”Ÿæˆå›å¤...", state="running")
            prompt = ChatPromptTemplate.from_template("ç”¨æˆ·è¯´: {question}\nè¯·ç”¨ç®€ç»ƒã€å‹å¥½çš„è¯­æ°”å›å¤ã€‚"
                                                      "åœ¨å›ç­”æ•°å€¼è®¡ç®—é—®é¢˜æ—¶ï¼Œä¸¥æ ¼æ ¹æ®å·¥å…·è¿”å›çš„ç»“æœè¿›è¡Œåˆ¤æ–­ï¼Œè‡ªå·±åˆ«ä¹±è¯´"
                                                      )
            chain = prompt | llm
            response_text = chain.invoke({"question": user_input}).content

        status.update(label="âœ… å®Œæˆ", state="complete", expanded=False)

    # [æ˜¾ç¤ºåŠ©æ‰‹å›å¤]
    with st.chat_message("assistant"):
        st.markdown(response_text)
        if fig:
            st.pyplot(fig) # â˜…â˜…â˜… å¦‚æœæœ‰å›¾ï¼Œåœ¨è¿™é‡Œæ˜¾ç¤º â˜…â˜…â˜…

    # [ä¿å­˜æ¶ˆæ¯åˆ° Session]
    st.session_state.messages.append({"role": "assistant", "content": response_text})
    
    # [è‡ªåŠ¨æŒä¹…åŒ–ä¿å­˜]
    # è°ƒç”¨ history_utils æŠŠå½“å‰å®Œæ•´å¯¹è¯å­˜å…¥ JSON
    history_utils.save_conversation(st.session_state.session_id, st.session_state.messages)
    
    # å¦‚æœæ˜¯æ–°å¯¹è¯ï¼ˆç¬¬ä¸€è½®äº¤äº’ï¼‰ï¼Œåˆ·æ–°ä¸€ä¸‹è®©ä¾§è¾¹æ å‡ºç°æ ‡é¢˜
    if len(st.session_state.messages) <= 2:
        st.rerun()