import os
import glob
import shutil
import time
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_community.document_loaders import TextLoader
# æ ¸å¿ƒç»„ä»¶ï¼šç»“æ„åŒ–åˆ‡åˆ†å™¨
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

# =================é…ç½®åŒºåŸŸ=================
PERSIST_DIRECTORY = "./chroma_db"
# ç¡®ä¿è¿™é‡ŒæŒ‡å‘ä½ å­˜æ”¾ DeepSeek æ¸…æ´—å Markdown æ–‡ä»¶çš„ç›®å½•
DATA_DIRECTORY = "./data" 
BATCH_SIZE = 30
# EMBEDDING_MODEL = "nomic-embed-text"
EMBEDDING_MODEL = "BAAI/bge-m3"
# =========================================

def intelligent_chunking(documents):
    """
    ã€æ ¸å¿ƒå‡çº§ã€‘ç»“æ„åŒ–è¯­ä¹‰åˆ‡åˆ† + ä¸Šä¸‹æ–‡æ³¨å…¥
    å®ç°é¢è¯•ä¸­æåˆ°çš„ "Structure-aware Semantic Chunking"
    """
    print(f"ğŸ”ª [Chunking] å¼€å§‹å¯¹ {len(documents)} ä»½æ–‡æ¡£è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†...")
    final_chunks = []
    
    # 1. å®šä¹‰ Markdown æ ‡é¢˜å±‚çº§ (DeepSeek æ¸…æ´—åçš„æ•°æ®é€šå¸¸åŒ…å«è¿™äº›)
    headers_to_split_on = [
        ("#", "Title"),      # ä¸€çº§æ ‡é¢˜
        ("##", "Section"),   # äºŒçº§æ ‡é¢˜ (ç« èŠ‚)
        ("###", "Subsection"), # ä¸‰çº§æ ‡é¢˜ (å°èŠ‚)
    ]
    
    # 2. åˆå§‹åŒ–åˆ‡åˆ†å™¨
    # é€»è¾‘å±‚ï¼šæŒ‰ Markdown ç»“æ„åˆ‡
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    # ç‰©ç†å±‚ï¼šå¤„ç†è¶…é•¿æ®µè½çš„å…œåº•æ–¹æ¡ˆ (çª—å£å¤§å°ç•¥å¤§äº Embedding é™åˆ¶)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,       
        chunk_overlap=50,     
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""] 
    )

    for doc in documents:
        # è·å–åŸå§‹å†…å®¹å’Œæºæ–‡ä»¶å
        content = doc.page_content
        source = doc.metadata.get("source", "unknown")
        
        # Step 1: æŒ‰ Markdown ç»“æ„ç²—åˆ‡
        # è¿™ä¸€æ­¥å‡ºæ¥çš„ chunk ä¼šè‡ªåŠ¨å¸¦æœ‰ metadata={'Section': '...', 'Title': '...'}
        md_header_splits = markdown_splitter.split_text(content)

        # Step 2: éå†ç²—åˆ‡åçš„ç‰‡æ®µï¼Œè¿›è¡Œç»†åˆ‡å’Œä¸Šä¸‹æ–‡æ³¨å…¥
        for split in md_header_splits:
            # ç»§æ‰¿æºæ–‡ä»¶å
            split.metadata["source"] = source
            
            # å¦‚æœç‰‡æ®µæœ¬èº«å°±å¾ˆå° (æ¯”å¦‚ < 800 å­—ç¬¦)ï¼Œä¸ç”¨å†åˆ‡ï¼Œä¿æŒé€»è¾‘å®Œæ•´æ€§
            if len(split.page_content) < 800:
                sub_splits = [split]
            else:
                # è¶…é•¿ç‰‡æ®µï¼Œè¿›è¡Œæ»‘åŠ¨çª—å£ç»†åˆ‡
                sub_splits = text_splitter.split_documents([split])
            
            # Step 3: â˜…â˜…â˜… å…ƒæ•°æ®æ³¨å…¥ (Metadata Injection) â˜…â˜…â˜…
            for sub_split in sub_splits:
                # ä» metadata æå–æ ‡é¢˜ç»“æ„
                title = sub_split.metadata.get("Title", "")
                section = sub_split.metadata.get("Section", "")
                subsection = sub_split.metadata.get("Subsection", "")
                
                # æ„é€ ä¸Šä¸‹æ–‡å‰ç¼€ (é¢åŒ…å±‘å¯¼èˆª)
                # æ ¼å¼ç¤ºä¾‹ï¼šã€æ–‡æ¡£ï¼šæ··æ²Œç†è®ºã€‘ã€ç« èŠ‚ï¼šLogisticæ˜ å°„ã€‘
                context_prefix = ""
                if title: context_prefix += f"ã€ä¸»é¢˜: {title}ã€‘"
                if section: context_prefix += f"ã€ç« èŠ‚: {section}ã€‘"
                if subsection: context_prefix += f"ã€å°èŠ‚: {subsection}ã€‘"
                
                # å°†ä¸Šä¸‹æ–‡æ‹¼æ¥åˆ°æ­£æ–‡å¤´éƒ¨
                # è¿™æ · Embedding å‘é‡å°±ä¼šåŒ…å«è¿™äº›å±‚çº§ä¿¡æ¯ï¼Œæ£€ç´¢å‡†ç¡®ç‡å¤§å¹…æå‡
                if context_prefix:
                    sub_split.page_content = f"{context_prefix}\n{sub_split.page_content}"
                
                final_chunks.append(sub_split)

    print(f"âœ… [Chunking] åˆ‡åˆ†å®Œæˆï¼Œç”Ÿæˆ {len(final_chunks)} ä¸ªè¯­ä¹‰ç‰‡æ®µ (å·²æ³¨å…¥ä¸Šä¸‹æ–‡å…ƒæ•°æ®)ã€‚")
    return final_chunks

def build_vector_db():
    print("ğŸš€ å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“ ...")
    
    # 1. å¼ºåˆ¶æ¸…ç©ºæ—§æ•°æ®åº“ (é˜²æ­¢æ—§çš„åƒåœ¾åˆ‡ç‰‡æ®‹ç•™)
    if os.path.exists(PERSIST_DIRECTORY):
        print(f"ğŸ—‘ï¸ æ£€æµ‹åˆ°æ—§æ•°æ®åº“ {PERSIST_DIRECTORY}ï¼Œæ­£åœ¨åˆ é™¤é‡å»º...")
        try:
            shutil.rmtree(PERSIST_DIRECTORY)
            time.sleep(1) # æ­‡ä¸€ç§’ï¼Œé˜²æ­¢ Windows æ–‡ä»¶å ç”¨æŠ¥é”™
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤å¤±è´¥: {e}ï¼Œå°è¯•ç»§ç»­...")

    # 2. è¿æ¥ Embedding
    print(f"ğŸ”Œ è¿æ¥ BGE-M3 æ¨¡å‹: {EMBEDDING_MODEL}...")
    try:
        # æ˜¾å¼æŒ‡å®š device='cpu' ä»¥èŠ‚çœæ˜¾å­˜
        # å¼€å¯ normalize_embeddings ä»¥ä¼˜åŒ–ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢
        embeddings = HuggingFaceBgeEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'}, 
            encode_kwargs={'normalize_embeddings': True}
        )
        # ç®€å•æµ‹è¯•ä¸€ä¸‹ï¼Œè§¦å‘æ¨¡å‹ä¸‹è½½ï¼ˆå¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œï¼‰
        embeddings.embed_query("test")
        print("âœ… BGE-M3 æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥sentence-transformersæ˜¯å¦å®‰è£…: {e}")
        return

    # 3. åŠ è½½ Markdown æ–‡ä»¶
    # ä¼˜å…ˆåŠ è½½ .mdï¼Œå› ä¸ºé‚£æ˜¯ DeepSeek æ¸…æ´—åçš„ç²¾å
    docs = []
    files = glob.glob(os.path.join(DATA_DIRECTORY, "*.md")) + glob.glob(os.path.join(DATA_DIRECTORY, "*.txt"))
    
    print(f"ğŸ“‚ å‘ç° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶ (.md/.txt)")
    if len(files) == 0:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼è¯·ç¡®ä¿ ./data ç›®å½•ä¸‹æœ‰æ¸…æ´—å¥½çš„ Markdown æ–‡ä»¶ã€‚")
        return

    for file_path in files:
        try:
            loader = TextLoader(file_path, encoding='utf-8')
            loaded_docs = loader.load()
            # è®°å½•æ–‡ä»¶åå…ƒæ•°æ®
            for doc in loaded_docs:
                doc.metadata["source"] = os.path.basename(file_path)
            docs.extend(loaded_docs)
            print(f"  - å·²åŠ è½½: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"  - âŒ åŠ è½½å¤±è´¥ {file_path}: {e}")

    # 4. æ‰§è¡Œæ™ºèƒ½åˆ‡åˆ† (æ›¿ä»£åŸæ¥çš„ TextSplitter)
    if not docs:
        return
        
    chunks = intelligent_chunking(docs)

    # 5. å†™å…¥æ•°æ®åº“
    print(f"ğŸ’¾ å¼€å§‹å†™å…¥ Chroma (Batch Size = {BATCH_SIZE})...")
    vectorstore = Chroma(
        embedding_function=embeddings,
        persist_directory=PERSIST_DIRECTORY,
        collection_name="chaos_science_db"
    )

    # åˆ†æ‰¹å†™å…¥ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡æ•ˆæœ
    total_chunks = len(chunks)
    for i in range(0, total_chunks, BATCH_SIZE):
        batch = chunks[i : i + BATCH_SIZE]
        try:
            vectorstore.add_documents(batch)
            # ç®€å•çš„è¿›åº¦æ‰“å°
            progress = ((i + len(batch)) / total_chunks) * 100
            print(f"\r  - å†™å…¥è¿›åº¦: {progress:.1f}% ({i + len(batch)}/{total_chunks})", end="")
        except Exception as e:
            print(f"\n  âš ï¸ æ‰¹æ¬¡å†™å…¥å¤±è´¥: {e}")

    print("\n\nçŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
    # print("ğŸ‘‰ ä½ çš„æ•°æ®ç°åœ¨æ‹¥æœ‰äº†ã€ç»“æ„åŒ–ä¸Šä¸‹æ–‡ã€‘ï¼Œå¿«å» app.py æé—®è¯•è¯•ï¼")

if __name__ == "__main__":
    build_vector_db()
